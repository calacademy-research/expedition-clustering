{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33cd140e-90ab-4c4b-8222-6e975bf7492f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "import Levenshtein as lev\n",
    "\n",
    "df = pd.read_csv(\"cel_med.csv\", on_bad_lines = \"skip\")\n",
    "\n",
    "\n",
    "def compute_levenshtein_matrix(strings):\n",
    "    n = len(strings)\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distance_matrix[i, j] = lev.distance(strings[i], strings[j])\n",
    "    return distance_matrix\n",
    "\n",
    "min_date = '1677-09-21'\n",
    "max_date = '2262-04-11'\n",
    "\n",
    "\n",
    "def filter_dates(date_str):\n",
    "    try:\n",
    "        date = pd.to_datetime(date_str)\n",
    "        if date < pd.to_datetime(min_date) or date > pd.to_datetime(max_date):\n",
    "            return pd.NaT\n",
    "        return date\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "df['startdate'] = df['startdate'].apply(filter_dates)\n",
    "df['enddate'] = df['enddate'].apply(filter_dates)\n",
    "\n",
    "\n",
    "df['startdate_num'] = df['startdate'].view('int64') / 1e9\n",
    "df['enddate_num'] = df['enddate'].view('int64') / 1e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350cb35a-21d0-41b1-8004-5ff88fc5d6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "locality_names = df['localityname'].fillna(\"\").tolist()\n",
    "lev_distance_matrix = compute_levenshtein_matrix(locality_names)\n",
    "\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=2024)\n",
    "lev_features = mds.fit_transform(lev_distance_matrix)\n",
    "\n",
    "\n",
    "df['lev_feature_1'] = lev_features[:, 0]\n",
    "df['lev_feature_2'] = lev_features[:, 1]\n",
    "\n",
    "features = df[['latitude1', 'longitude1', 'lev_feature_1', 'lev_feature_2', 'startdate_num', 'enddate_num']]\n",
    "\n",
    "features = features.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd776a53-50ca-4ec8-8703-71c98adac4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.1, min_samples=3)\n",
    "clusters = dbscan.fit_predict(features_scaled)\n",
    "\n",
    "df['cluster'] = -1 \n",
    "df.loc[features.index, 'cluster'] = clusters\n",
    "\n",
    "def create_convex_hulls(df):\n",
    "    polygons = []\n",
    "    for cluster in df['cluster'].unique():\n",
    "        if cluster == -1:\n",
    "            continue\n",
    "        points = df[df['cluster'] == cluster][['longitude1', 'latitude1']].values\n",
    "        if len(points) < 3:\n",
    "            continue\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            vertices = points[hull.vertices]\n",
    "            polygon = Polygon(vertices)\n",
    "            polygons.append({'cluster': cluster, 'geometry': polygon})\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90b59a-21c9-497b-9d93-e688769119e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_long_bounds = [-125, -66.93457]\n",
    "us_lat_bounds = [24.396308, 49.384358]\n",
    "data_us = df[\n",
    "    (df['longitude1'] >= us_long_bounds[0]) & (df['longitude1'] <= us_long_bounds[1]) &\n",
    "    (df['latitude1'] >= us_lat_bounds[0]) & (df['latitude1'] <= us_lat_bounds[1])\n",
    "]\n",
    "\n",
    "polygons = create_convex_hulls(data_us)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(polygons)\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "us = world[(world.name == \"United States of America\")]\n",
    "\n",
    "us = us.cx[us_long_bounds[0]:us_long_bounds[1], us_lat_bounds[0]:us_lat_bounds[1]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "us.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf.plot(ax=ax, column='cluster', cmap='tab20', legend=True, alpha=0.5)\n",
    "\n",
    "ax.set_xlim(us_long_bounds)\n",
    "ax.set_ylim(us_lat_bounds)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('DBSCAN Clusters as Regions (Polygons) Overlaid on Continental US Map')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
